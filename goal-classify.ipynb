{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.scroll_flag = true\n",
       "window.scroll_exit = false\n",
       "window.scroll_delay = 100\n",
       "\n",
       "$(\".output_scroll\").each(function() {\n",
       "    $(this)[0].scrollTop = $(this)[0].scrollHeight;\n",
       "});\n",
       "\n",
       "function callScrollToBottom() {\n",
       "    setTimeout(scrollToBottom, window.scroll_delay);\n",
       "}\n",
       "\n",
       "function scrollToBottom() {\n",
       "    if (window.scroll_exit) {\n",
       "        return;\n",
       "    }\n",
       "    if (!window.scroll_flag) {\n",
       "        callScrollToBottom();\n",
       "        return;\n",
       "    };\n",
       "\n",
       "    $(\".output_scroll\").each(function() {\n",
       "        if (!$(this).attr('scroll_checkbox')){\n",
       "            window.scroll_flag = true;\n",
       "            $(this).attr('scroll_checkbox',true);\n",
       "            var div = document.createElement('div');\n",
       "            var checkbox = document.createElement('input');\n",
       "            checkbox.type = \"checkbox\";\n",
       "            checkbox.onclick = function(){window.scroll_flag = checkbox.checked}\n",
       "            checkbox.checked = \"checked\"\n",
       "            div.append(\"Auto-Scroll-To-Bottom: \");\n",
       "            div.append(checkbox);\n",
       "            $(this).parent().before(div);\n",
       "        }\n",
       "\n",
       "        $(this)[0].scrollTop = $(this)[0].scrollHeight;\n",
       "    });\n",
       "    callScrollToBottom();\n",
       "}\n",
       "scrollToBottom();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.scroll_flag = true\n",
    "window.scroll_exit = false\n",
    "window.scroll_delay = 100\n",
    "\n",
    "$(\".output_scroll\").each(function() {\n",
    "    $(this)[0].scrollTop = $(this)[0].scrollHeight;\n",
    "});\n",
    "\n",
    "function callScrollToBottom() {\n",
    "    setTimeout(scrollToBottom, window.scroll_delay);\n",
    "}\n",
    "\n",
    "function scrollToBottom() {\n",
    "    if (window.scroll_exit) {\n",
    "        return;\n",
    "    }\n",
    "    if (!window.scroll_flag) {\n",
    "        callScrollToBottom();\n",
    "        return;\n",
    "    };\n",
    "\n",
    "    $(\".output_scroll\").each(function() {\n",
    "        if (!$(this).attr('scroll_checkbox')){\n",
    "            window.scroll_flag = true;\n",
    "            $(this).attr('scroll_checkbox',true);\n",
    "            var div = document.createElement('div');\n",
    "            var checkbox = document.createElement('input');\n",
    "            checkbox.type = \"checkbox\";\n",
    "            checkbox.onclick = function(){window.scroll_flag = checkbox.checked}\n",
    "            checkbox.checked = \"checked\"\n",
    "            div.append(\"Auto-Scroll-To-Bottom: \");\n",
    "            div.append(checkbox);\n",
    "            $(this).parent().before(div);\n",
    "        }\n",
    "\n",
    "        $(this)[0].scrollTop = $(this)[0].scrollHeight;\n",
    "    });\n",
    "    callScrollToBottom();\n",
    "}\n",
    "scrollToBottom();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Cristano_Ronaldo_Final_v1/data.csv\")\n",
    "data = data.drop(data.columns[0],axis=1)\n",
    "# data.head()\n",
    "\n",
    "# purged features:\n",
    "data = data.drop(['shot_id_number'],axis=1)\n",
    "data = data.drop(['date_of_game'],axis=1)\n",
    "data = data.drop(['team_id'],axis=1)\n",
    "# data = data.drop(['type_of_shot'],axis=1)\n",
    "# data = data.drop(['game_season'], axis=1)\n",
    "# data = data[data['shot_id_number'].notnull()]\n",
    "\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# data = data.drop([\"type_of_shot_shot - 37\",\"home/away_MANU @ NOK\",\"shot_basics_Right Corner\",\"home/away_MANU vs. NOP\",\"lat/lng_40.324211, -111.674849\",\"type_of_shot_shot - 33\",\n",
    "# \"home/away_MANU vs. VAN\",\n",
    "# \"type_of_combined_shot_shot - 2\",\n",
    "# \"shot_basics_Left Corner\",\n",
    "# \"type_of_shot_shot - 34\",\n",
    "# \"home/away_MANU vs. CHH\",\n",
    "# \"home/away_MANU @ VAN\",\n",
    "# \"lat/lng_49.250068, -123.114646\",\n",
    "# \"lat/lng_30.055498, -89.960838\",\n",
    "# \"lat/lng_35.205878, -80.841194\",\n",
    "# \"home/away_MANU @ NOP\",\n",
    "# \"home/away_MANU @ CHH\",\n",
    "# \"home/away_MANU vs. PHO\",\n",
    "# \"range_of_shot_Back Court Shot\",\n",
    "# \"area_of_shot_Mid Ground(MG)\",\n",
    "# \"game_season_2013-14\",\n",
    "# \"home/away_MANU vs. SAN\",\n",
    "# \"lat/lng_33.513157, -112.082793\",\n",
    "# \"home/away_MANU @ PHO\",\n",
    "# \"home/away_MANU @ BKN\",\n",
    "# \"lat/lng_40.623199, -73.951223\",\n",
    "# \"shot_basics_Mid Ground Line\",\n",
    "# \"home/away_MANU @ UTH\",\n",
    "# \"home/away_MANU vs. BKN\"],axis=1)\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local train size: 20798\n",
      "local validation size: 3631\n",
      "test size: 6268\n"
     ]
    }
   ],
   "source": [
    "#######LOCAL TRAIN-VALIDATION SPLIT########\n",
    "#Rescale all feature values:\n",
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "#global train-test split:\n",
    "train = data[data['is_goal'].notnull()]\n",
    "y = train['is_goal']\n",
    "test = data[data['is_goal'].isnull()]\n",
    "\n",
    "# local training / validation performance:\n",
    "train['local_train']=np.random.uniform(0,1,len(train))<=0.85\n",
    "local_train, local_validation = train[train['local_train']==True], train[train['local_train']==False]\n",
    "y_local_train = local_train['is_goal']\n",
    "y_local_validation=local_validation['is_goal']\n",
    "local_train=local_train.drop(['is_goal'],axis=1)\n",
    "local_validation =local_validation.drop(['is_goal'],axis=1)\n",
    "\n",
    "features = local_train.columns[:-1]\n",
    "# print(features)\n",
    "local_train = local_train[features]\n",
    "local_validation = local_validation[features]\n",
    "print(\"local train size:\", len(local_train))\n",
    "print(\"local validation size:\", len(local_validation))\n",
    "print(\"test size:\", len(test))\n",
    "\n",
    "# preprocessing local-train+validation:\n",
    "imp= SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "imp = imp.fit(local_train)\n",
    "local_train_modified = imp.transform(local_train)\n",
    "\n",
    "#scaling\n",
    "std_scaler.fit(local_train_modified)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "local_train_modified = std_scaler.transform(local_train_modified)\n",
    "imp = imp.fit(local_validation)\n",
    "local_validation_modified = imp.transform(local_validation)\n",
    "local_validation_modified = std_scaler.transform(local_validation_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation a/c: 55.27402919305976\n",
      "score: 1.6837123607036255e-07\n",
      "                                importance\n",
      "match_id                          0.056892\n",
      "match_event_id                    0.056813\n",
      "remaining_sec.1                   0.053236\n",
      "remaining_sec                     0.051436\n",
      "location_y                        0.051244\n",
      "location_x                        0.049600\n",
      "remaining_min.1                   0.044280\n",
      "distance_of_shot.1                0.042888\n",
      "distance_of_shot                  0.038160\n",
      "remaining_min                     0.036731\n",
      "power_of_shot.1                   0.033929\n",
      "knockout_match.1                  0.026112\n",
      "power_of_shot                     0.021277\n",
      "lat/lng_42.982923, -71.446094     0.010382\n",
      "knockout_match                    0.008876\n",
      "type_of_combined_shot_shot - 3    0.008781\n",
      "type_of_shot_shot - 39            0.008776\n",
      "shot_basics_Goal Area             0.007720\n",
      "type_of_combined_shot_shot - 1    0.007697\n",
      "area_of_shot_Center(C)            0.007170\n",
      "range_of_shot_Less Than 8 ft.     0.006992\n",
      "type_of_shot_shot - 44            0.005478\n",
      "game_season_2005-06               0.004948\n",
      "game_season_2008-09               0.004937\n",
      "game_season_2007-08               0.004779\n",
      "game_season_2009-10               0.004751\n",
      "team_name_Manchester United       0.004751\n",
      "game_season_2002-03               0.004720\n",
      "game_season_2001-02               0.004540\n",
      "shot_basics_Mid Range             0.004530\n",
      "...                                    ...\n",
      "type_of_shot_shot - 40            0.000634\n",
      "type_of_shot_shot - 28            0.000632\n",
      "home/away_MANU vs. VAN            0.000628\n",
      "type_of_shot_shot - 37            0.000616\n",
      "type_of_shot_shot - 33            0.000579\n",
      "type_of_shot_shot - 34            0.000560\n",
      "shot_basics_Right Corner          0.000553\n",
      "type_of_combined_shot_shot - 2    0.000531\n",
      "lat/lng_49.250068, -123.114646    0.000527\n",
      "home/away_MANU @ VAN              0.000511\n",
      "shot_basics_Left Corner           0.000508\n",
      "home/away_MANU vs. CHH            0.000482\n",
      "home/away_MANU @ NOP              0.000392\n",
      "lat/lng_30.055498, -89.960838     0.000379\n",
      "lat/lng_35.205878, -80.841194     0.000355\n",
      "home/away_MANU @ CHH              0.000350\n",
      "area_of_shot_Mid Ground(MG)       0.000295\n",
      "home/away_MANU vs. PHO            0.000291\n",
      "game_season_2013-14               0.000290\n",
      "range_of_shot_Back Court Shot     0.000269\n",
      "home/away_MANU vs. SAN            0.000242\n",
      "lat/lng_33.513157, -112.082793    0.000202\n",
      "home/away_MANU @ PHO              0.000201\n",
      "home/away_MANU @ BKN              0.000180\n",
      "shot_basics_Mid Ground Line       0.000175\n",
      "lat/lng_40.623199, -73.951223     0.000171\n",
      "home/away_MANU @ UTH              0.000161\n",
      "home/away_MANU vs. BKN            0.000117\n",
      "lat/lng_40.324211, -111.674849    0.000113\n",
      "home/away_MANU vs. NOP            0.000092\n",
      "\n",
      "[228 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#RF classifier for train-validation perf:\n",
    "clf = RandomForestClassifier(n_jobs=2, n_estimators=100, random_state=0)\n",
    "clf.fit(local_train_modified, y_local_train)\n",
    "p = clf.predict_proba(local_validation_modified)\n",
    "y_validation_pred = []\n",
    "for x,y in p:\n",
    "    y_validation_pred.append(y)\n",
    "count_match = 0\n",
    "count_error = 0\n",
    "deviation = 0.0\n",
    "assert(len(y_validation_pred)==len(y_local_validation))\n",
    "validation_gtruth=np.asarray(y_local_validation)\n",
    "for i in range(len(y_local_validation)):\n",
    "    deviation += abs(y_validation_pred[i]-validation_gtruth[i])\n",
    "    if (int(y_validation_pred[i])==int(validation_gtruth[i])):\n",
    "        count_match+=1\n",
    "    else:\n",
    "        count_error+=1\n",
    "validation_accuracy = count_match/(count_match+count_error)*100.0\n",
    "print(\"validation a/c:\", validation_accuracy)\n",
    "print(\"score:\", 1.0/(1.0+deviation/1.0*(count_match+count_error)))\n",
    "features_imp = pd.DataFrame(clf.feature_importances_, index=features,columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(features_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######ACTUAL TRAIN-TEST PERDICTION########\n",
    "y_train = train['is_goal']\n",
    "train=train.drop(['is_goal'],axis=1)\n",
    "test =test.drop(['is_goal'],axis=1)\n",
    "features = local_train.columns[:-1]\n",
    "train = train[features]\n",
    "test = test[features]\n",
    "print(\"Train size:\", len(train))\n",
    "print(\"Test size:\", len(test))\n",
    "\n",
    "# preprocessing local-train+validation:\n",
    "imp= SimpleImputer(missing_values=np.nan,strategy='mean')\n",
    "imp = imp.fit(train)\n",
    "train_modified = imp.transform(train)\n",
    "imp = imp.fit(test)\n",
    "test_modified = imp.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69236860\n",
      "Iteration 2, loss = 0.67354029\n",
      "Iteration 3, loss = 0.66293003\n",
      "Iteration 4, loss = 0.65479031\n",
      "Iteration 5, loss = 0.64879819\n",
      "Iteration 6, loss = 0.64436252\n",
      "Iteration 7, loss = 0.64083090\n",
      "Iteration 8, loss = 0.63807425\n",
      "Iteration 9, loss = 0.63563544\n",
      "Iteration 10, loss = 0.63312026\n",
      "Iteration 11, loss = 0.63130469\n",
      "Iteration 12, loss = 0.62955053\n",
      "Iteration 13, loss = 0.62745305\n",
      "Iteration 14, loss = 0.62611035\n",
      "Iteration 15, loss = 0.62448870\n",
      "Iteration 16, loss = 0.62278500\n",
      "Iteration 17, loss = 0.62134776\n",
      "Iteration 18, loss = 0.62001036\n",
      "Iteration 19, loss = 0.61835805\n",
      "Iteration 20, loss = 0.61714664\n",
      "Iteration 21, loss = 0.61573032\n",
      "Iteration 22, loss = 0.61430002\n",
      "Iteration 23, loss = 0.61272679\n",
      "Iteration 24, loss = 0.61159315\n",
      "Iteration 25, loss = 0.61005431\n",
      "Iteration 26, loss = 0.60862868\n",
      "Iteration 27, loss = 0.60720879\n",
      "Iteration 28, loss = 0.60593288\n",
      "Iteration 29, loss = 0.60453354\n",
      "Iteration 30, loss = 0.60323390\n",
      "Iteration 31, loss = 0.60207998\n",
      "Iteration 32, loss = 0.60055306\n",
      "Iteration 33, loss = 0.59893952\n",
      "Iteration 34, loss = 0.59775388\n",
      "Iteration 35, loss = 0.59641685\n",
      "Iteration 36, loss = 0.59574601\n",
      "Iteration 37, loss = 0.59370178\n",
      "Iteration 38, loss = 0.59308250\n",
      "Iteration 39, loss = 0.59120285\n",
      "Iteration 40, loss = 0.59036529\n",
      "Iteration 41, loss = 0.58895534\n",
      "Iteration 42, loss = 0.58791474\n",
      "Iteration 43, loss = 0.58721134\n",
      "Iteration 44, loss = 0.58577003\n",
      "Iteration 45, loss = 0.58442386\n",
      "Iteration 46, loss = 0.58374595\n",
      "Iteration 47, loss = 0.58236572\n",
      "Iteration 48, loss = 0.58172909\n",
      "Iteration 49, loss = 0.58048015\n",
      "Iteration 50, loss = 0.57927152\n",
      "Iteration 51, loss = 0.57847061\n",
      "Iteration 52, loss = 0.57726123\n",
      "Iteration 53, loss = 0.57608331\n",
      "Iteration 54, loss = 0.57522417\n",
      "Iteration 55, loss = 0.57429032\n",
      "Iteration 56, loss = 0.57280100\n",
      "Iteration 57, loss = 0.57162218\n",
      "Iteration 58, loss = 0.57095792\n",
      "Iteration 59, loss = 0.57065011\n",
      "Iteration 60, loss = 0.56871951\n",
      "Iteration 61, loss = 0.56858201\n",
      "Iteration 62, loss = 0.56738108\n",
      "Iteration 63, loss = 0.56603989\n",
      "Iteration 64, loss = 0.56583072\n",
      "Iteration 65, loss = 0.56440891\n",
      "Iteration 66, loss = 0.56358585\n",
      "Iteration 67, loss = 0.56272060\n",
      "Iteration 68, loss = 0.56130636\n",
      "Iteration 69, loss = 0.56136294\n",
      "Iteration 70, loss = 0.56029180\n",
      "Iteration 71, loss = 0.55929335\n",
      "Iteration 72, loss = 0.55936198\n",
      "Iteration 73, loss = 0.55728695\n",
      "Iteration 74, loss = 0.55721519\n",
      "Iteration 75, loss = 0.55647033\n",
      "Iteration 76, loss = 0.55659914\n",
      "Iteration 77, loss = 0.55589418\n",
      "Iteration 78, loss = 0.55610001\n",
      "Iteration 79, loss = 0.55387201\n",
      "Iteration 80, loss = 0.55284027\n",
      "Iteration 81, loss = 0.55351420\n",
      "Iteration 82, loss = 0.55215590\n",
      "Iteration 83, loss = 0.55190166\n",
      "Iteration 84, loss = 0.55177778\n",
      "Iteration 85, loss = 0.55070415\n",
      "Iteration 86, loss = 0.55146362\n",
      "Iteration 87, loss = 0.54914780\n",
      "Iteration 88, loss = 0.54964310\n",
      "Iteration 89, loss = 0.54760732\n",
      "Iteration 90, loss = 0.54685422\n",
      "Iteration 91, loss = 0.54666750\n",
      "Iteration 92, loss = 0.54639391\n",
      "Iteration 93, loss = 0.54748794\n",
      "Iteration 94, loss = 0.54463977\n",
      "Iteration 95, loss = 0.54525269\n",
      "Iteration 96, loss = 0.54572467\n",
      "Iteration 97, loss = 0.54413358\n",
      "Iteration 98, loss = 0.54273933\n",
      "Iteration 99, loss = 0.54393013\n",
      "Iteration 100, loss = 0.54189783\n",
      "Iteration 101, loss = 0.54139490\n",
      "Iteration 102, loss = 0.54144186\n",
      "Iteration 103, loss = 0.54081793\n",
      "Iteration 104, loss = 0.54060779\n",
      "Iteration 105, loss = 0.54069538\n",
      "Iteration 106, loss = 0.53922385\n",
      "Iteration 107, loss = 0.53874423\n",
      "Iteration 108, loss = 0.53787037\n",
      "Iteration 109, loss = 0.53849803\n",
      "Iteration 110, loss = 0.53788201\n",
      "Iteration 111, loss = 0.53866004\n",
      "Iteration 112, loss = 0.53774742\n",
      "Iteration 113, loss = 0.53711905\n",
      "Iteration 114, loss = 0.53655485\n",
      "Iteration 115, loss = 0.53572538\n",
      "Iteration 116, loss = 0.53418945\n",
      "Iteration 117, loss = 0.53402028\n",
      "Iteration 118, loss = 0.53469980\n",
      "Iteration 119, loss = 0.53368951\n",
      "Iteration 120, loss = 0.53622817\n",
      "Iteration 121, loss = 0.53474308\n",
      "Iteration 122, loss = 0.53345199\n",
      "Iteration 123, loss = 0.53281380\n",
      "Iteration 124, loss = 0.53400189\n",
      "Iteration 125, loss = 0.53229830\n",
      "Iteration 126, loss = 0.53217129\n",
      "Iteration 127, loss = 0.53044618\n",
      "Iteration 128, loss = 0.53007870\n",
      "Iteration 129, loss = 0.53144768\n",
      "Iteration 130, loss = 0.52942015\n",
      "Iteration 131, loss = 0.53019795\n",
      "Iteration 132, loss = 0.52952800\n",
      "Iteration 133, loss = 0.52911379\n",
      "Iteration 134, loss = 0.53209856\n",
      "Iteration 135, loss = 0.52757217\n",
      "Iteration 136, loss = 0.52672058\n",
      "Iteration 137, loss = 0.52696520\n",
      "Iteration 138, loss = 0.52677444\n",
      "Iteration 139, loss = 0.52609737\n",
      "Iteration 140, loss = 0.52697697\n",
      "Iteration 141, loss = 0.52602336\n",
      "Iteration 142, loss = 0.52506827\n",
      "Iteration 143, loss = 0.52528941\n",
      "Iteration 144, loss = 0.52678973\n",
      "Iteration 145, loss = 0.52531149\n",
      "Iteration 146, loss = 0.52492562\n",
      "Iteration 147, loss = 0.52546617\n",
      "Iteration 148, loss = 0.52347363\n",
      "Iteration 149, loss = 0.52327957\n",
      "Iteration 150, loss = 0.52430177\n",
      "Iteration 151, loss = 0.52299618\n",
      "Iteration 152, loss = 0.52195850\n",
      "Iteration 153, loss = 0.52146298\n",
      "Iteration 154, loss = 0.52202513\n",
      "Iteration 155, loss = 0.52393076\n",
      "Iteration 156, loss = 0.51999681\n",
      "Iteration 157, loss = 0.52054962\n",
      "Iteration 158, loss = 0.52024416\n",
      "Iteration 159, loss = 0.51997181\n",
      "Iteration 160, loss = 0.52001544\n",
      "Iteration 161, loss = 0.51970619\n",
      "Iteration 162, loss = 0.52115436\n",
      "Iteration 163, loss = 0.51916509\n",
      "Iteration 164, loss = 0.51929488\n",
      "Iteration 165, loss = 0.51894586\n",
      "Iteration 166, loss = 0.51905619\n",
      "Iteration 167, loss = 0.51979222\n",
      "Iteration 168, loss = 0.51753438\n",
      "Iteration 169, loss = 0.51613770\n",
      "Iteration 170, loss = 0.51667784\n",
      "Iteration 171, loss = 0.51860050\n",
      "Iteration 172, loss = 0.51995115\n",
      "Iteration 173, loss = 0.51876664\n",
      "Iteration 174, loss = 0.51840786\n",
      "Iteration 175, loss = 0.51696130\n",
      "Iteration 176, loss = 0.51603007\n",
      "Iteration 177, loss = 0.51775870\n",
      "Iteration 178, loss = 0.51654333\n",
      "Iteration 179, loss = 0.51570705\n",
      "Iteration 180, loss = 0.51324178\n",
      "Iteration 181, loss = 0.51508030\n",
      "Iteration 182, loss = 0.51609736\n",
      "Iteration 183, loss = 0.51403747\n",
      "Iteration 184, loss = 0.51422377\n",
      "Iteration 185, loss = 0.51327377\n",
      "Iteration 186, loss = 0.51363120\n",
      "Iteration 187, loss = 0.51290816\n",
      "Iteration 188, loss = 0.51356321\n",
      "Iteration 189, loss = 0.51258522\n",
      "Iteration 190, loss = 0.51265914\n",
      "Iteration 191, loss = 0.51223777\n",
      "Iteration 192, loss = 0.51166498\n",
      "Iteration 193, loss = 0.51283002\n",
      "Iteration 194, loss = 0.51143846\n",
      "Iteration 195, loss = 0.51080838\n",
      "Iteration 196, loss = 0.51325680\n",
      "Iteration 197, loss = 0.51084212\n",
      "Iteration 198, loss = 0.51024558\n",
      "Iteration 199, loss = 0.50991041\n",
      "Iteration 200, loss = 0.51081320\n",
      "Iteration 201, loss = 0.50935568\n",
      "Iteration 202, loss = 0.50971239\n",
      "Iteration 203, loss = 0.51046689\n",
      "Iteration 204, loss = 0.51083045\n",
      "Iteration 205, loss = 0.51044586\n",
      "Iteration 206, loss = 0.50908124\n",
      "Iteration 207, loss = 0.50890497\n",
      "Iteration 208, loss = 0.50724679\n",
      "Iteration 209, loss = 0.50899706\n",
      "Iteration 210, loss = 0.50896778\n",
      "Iteration 211, loss = 0.50709424\n",
      "Iteration 212, loss = 0.50926943\n",
      "Iteration 213, loss = 0.50610489\n",
      "Iteration 214, loss = 0.50647201\n",
      "Iteration 215, loss = 0.50834184\n",
      "Iteration 216, loss = 0.50719759\n",
      "Iteration 217, loss = 0.50707015\n",
      "Iteration 218, loss = 0.50723797\n",
      "Iteration 219, loss = 0.50665705\n",
      "Iteration 220, loss = 0.50805529\n",
      "Iteration 221, loss = 0.50579158\n",
      "Iteration 222, loss = 0.50736845\n",
      "Iteration 223, loss = 0.50660278\n",
      "Iteration 224, loss = 0.50574228\n",
      "Iteration 225, loss = 0.50590527\n",
      "Iteration 226, loss = 0.50444000\n",
      "Iteration 227, loss = 0.50374671\n",
      "Iteration 228, loss = 0.50494622\n",
      "Iteration 229, loss = 0.50278359\n",
      "Iteration 230, loss = 0.50419626\n",
      "Iteration 231, loss = 0.50271018\n",
      "Iteration 232, loss = 0.50230642\n",
      "Iteration 233, loss = 0.50433806\n",
      "Iteration 234, loss = 0.50124237\n",
      "Iteration 235, loss = 0.50524732\n",
      "Iteration 236, loss = 0.50391856\n",
      "Iteration 237, loss = 0.50353891\n",
      "Iteration 238, loss = 0.50327005\n",
      "Iteration 239, loss = 0.50277675\n",
      "Iteration 240, loss = 0.50365019\n",
      "Iteration 241, loss = 0.50227570\n",
      "Iteration 242, loss = 0.50129566\n",
      "Iteration 243, loss = 0.50140498\n",
      "Iteration 244, loss = 0.50200678\n",
      "Iteration 245, loss = 0.50009610\n",
      "Iteration 246, loss = 0.50066382\n",
      "Iteration 247, loss = 0.49938366\n",
      "Iteration 248, loss = 0.50066893\n",
      "Iteration 249, loss = 0.49920103\n",
      "Iteration 250, loss = 0.50024994\n",
      "Iteration 251, loss = 0.49915493\n",
      "Iteration 252, loss = 0.50071077\n",
      "Iteration 253, loss = 0.49923536\n",
      "Iteration 254, loss = 0.49988956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 255, loss = 0.50000945\n",
      "Iteration 256, loss = 0.50260851\n",
      "Iteration 257, loss = 0.49930288\n",
      "Iteration 258, loss = 0.49978783\n",
      "Iteration 259, loss = 0.49881483\n",
      "Iteration 260, loss = 0.49667865\n",
      "Iteration 261, loss = 0.49675622\n",
      "Iteration 262, loss = 0.49904449\n",
      "Iteration 263, loss = 0.49990633\n",
      "Iteration 264, loss = 0.49923735\n",
      "Iteration 265, loss = 0.49707122\n",
      "Iteration 266, loss = 0.49675035\n",
      "Iteration 267, loss = 0.49813810\n",
      "Iteration 268, loss = 0.49655665\n",
      "Iteration 269, loss = 0.49570842\n",
      "Iteration 270, loss = 0.49597934\n",
      "Iteration 271, loss = 0.49683165\n",
      "Iteration 272, loss = 0.49534628\n",
      "Iteration 273, loss = 0.49748063\n",
      "Iteration 274, loss = 0.49620063\n",
      "Iteration 275, loss = 0.49599014\n",
      "Iteration 276, loss = 0.49593399\n",
      "Iteration 277, loss = 0.49599993\n",
      "Iteration 278, loss = 0.49568637\n",
      "Iteration 279, loss = 0.49466459\n",
      "Iteration 280, loss = 0.49363093\n",
      "Iteration 281, loss = 0.49583472\n",
      "Iteration 282, loss = 0.49617000\n",
      "Iteration 283, loss = 0.49265267\n",
      "Iteration 284, loss = 0.49524538\n",
      "Iteration 285, loss = 0.49441540\n",
      "Iteration 286, loss = 0.49375637\n",
      "Iteration 287, loss = 0.49529471\n",
      "Iteration 288, loss = 0.49276536\n",
      "Iteration 289, loss = 0.49298908\n",
      "Iteration 290, loss = 0.49541210\n",
      "Iteration 291, loss = 0.49474811\n",
      "Iteration 292, loss = 0.49289638\n",
      "Iteration 293, loss = 0.49379769\n",
      "Iteration 294, loss = 0.49411307\n",
      "Iteration 295, loss = 0.49280001\n",
      "Iteration 296, loss = 0.49350170\n",
      "Iteration 297, loss = 0.49356257\n",
      "Iteration 298, loss = 0.49131615\n",
      "Iteration 299, loss = 0.49280736\n",
      "Iteration 300, loss = 0.49352719\n",
      "Iteration 301, loss = 0.49283668\n",
      "Iteration 302, loss = 0.49302036\n",
      "Iteration 303, loss = 0.49161060\n",
      "Iteration 304, loss = 0.49371746\n",
      "Iteration 305, loss = 0.49347262\n",
      "Iteration 306, loss = 0.49238424\n",
      "Iteration 307, loss = 0.49256604\n",
      "Iteration 308, loss = 0.49344396\n",
      "Iteration 309, loss = 0.49069058\n",
      "Iteration 310, loss = 0.49105196\n",
      "Iteration 311, loss = 0.49156299\n",
      "Iteration 312, loss = 0.49054482\n",
      "Iteration 313, loss = 0.49379025\n",
      "Iteration 314, loss = 0.49075421\n",
      "Iteration 315, loss = 0.49113812\n",
      "Iteration 316, loss = 0.49009009\n",
      "Iteration 317, loss = 0.49061987\n",
      "Iteration 318, loss = 0.49056453\n",
      "Iteration 319, loss = 0.48984530\n",
      "Iteration 320, loss = 0.48905410\n",
      "Iteration 321, loss = 0.49117277\n",
      "Iteration 322, loss = 0.48907832\n",
      "Iteration 323, loss = 0.49089908\n",
      "Iteration 324, loss = 0.49083469\n",
      "Iteration 325, loss = 0.49185473\n",
      "Iteration 326, loss = 0.49162999\n",
      "Iteration 327, loss = 0.48925575\n",
      "Iteration 328, loss = 0.48989704\n",
      "Iteration 329, loss = 0.49230864\n",
      "Iteration 330, loss = 0.48990503\n",
      "Iteration 331, loss = 0.48908954\n",
      "Iteration 332, loss = 0.48815186\n",
      "Iteration 333, loss = 0.48748154\n",
      "Iteration 334, loss = 0.48895009\n",
      "Iteration 335, loss = 0.48897311\n",
      "Iteration 336, loss = 0.48788707\n",
      "Iteration 337, loss = 0.48961068\n",
      "Iteration 338, loss = 0.48714988\n",
      "Iteration 339, loss = 0.48835195\n",
      "Iteration 340, loss = 0.48925685\n",
      "Iteration 341, loss = 0.48727688\n",
      "Iteration 342, loss = 0.48821249\n",
      "Iteration 343, loss = 0.48886100\n",
      "Iteration 344, loss = 0.48871011\n",
      "Iteration 345, loss = 0.48989721\n",
      "Iteration 346, loss = 0.48803586\n",
      "Iteration 347, loss = 0.48783621\n",
      "Iteration 348, loss = 0.48657821\n",
      "Iteration 349, loss = 0.48861721\n",
      "Iteration 350, loss = 0.48941076\n",
      "Iteration 351, loss = 0.48745268\n",
      "Iteration 352, loss = 0.48605726\n",
      "Iteration 353, loss = 0.48734253\n",
      "Iteration 354, loss = 0.48765312\n",
      "Iteration 355, loss = 0.48736398\n",
      "Iteration 356, loss = 0.48832764\n",
      "Iteration 357, loss = 0.48867976\n",
      "Iteration 358, loss = 0.48721304\n",
      "Iteration 359, loss = 0.48509545\n",
      "Iteration 360, loss = 0.48556343\n",
      "Iteration 361, loss = 0.48722465\n",
      "Iteration 362, loss = 0.48535309\n",
      "Iteration 363, loss = 0.48592280\n",
      "Iteration 364, loss = 0.48785879\n",
      "Iteration 365, loss = 0.48752025\n",
      "Iteration 366, loss = 0.48575057\n",
      "Iteration 367, loss = 0.48421464\n",
      "Iteration 368, loss = 0.48598719\n",
      "Iteration 369, loss = 0.48384742\n",
      "Iteration 370, loss = 0.48512204\n",
      "Iteration 371, loss = 0.48541354\n",
      "Iteration 372, loss = 0.48719586\n",
      "Iteration 373, loss = 0.48384568\n",
      "Iteration 374, loss = 0.48557071\n",
      "Iteration 375, loss = 0.48502267\n",
      "Iteration 376, loss = 0.48512815\n",
      "Iteration 377, loss = 0.48374475\n",
      "Iteration 378, loss = 0.48917884\n",
      "Iteration 379, loss = 0.48358638\n",
      "Iteration 380, loss = 0.48460290\n",
      "Iteration 381, loss = 0.48546993\n",
      "Iteration 382, loss = 0.48462756\n",
      "Iteration 383, loss = 0.48325500\n",
      "Iteration 384, loss = 0.48280946\n",
      "Iteration 385, loss = 0.48345429\n",
      "Iteration 386, loss = 0.48293571\n",
      "Iteration 387, loss = 0.48314213\n",
      "Iteration 388, loss = 0.48353367\n",
      "Iteration 389, loss = 0.48293666\n",
      "Iteration 390, loss = 0.48247046\n",
      "Iteration 391, loss = 0.48247805\n",
      "Iteration 392, loss = 0.48459773\n",
      "Iteration 393, loss = 0.48450361\n",
      "Iteration 394, loss = 0.48296688\n",
      "Iteration 395, loss = 0.48171988\n",
      "Iteration 396, loss = 0.48245826\n",
      "Iteration 397, loss = 0.48318764\n",
      "Iteration 398, loss = 0.48177547\n",
      "Iteration 399, loss = 0.48204508\n",
      "Iteration 400, loss = 0.48179510\n",
      "Iteration 401, loss = 0.48403129\n",
      "Iteration 402, loss = 0.48267665\n",
      "Iteration 403, loss = 0.48104607\n",
      "Iteration 404, loss = 0.48151537\n",
      "Iteration 405, loss = 0.48353111\n",
      "Iteration 406, loss = 0.48330342\n",
      "Iteration 407, loss = 0.48157523\n",
      "Iteration 408, loss = 0.47986197\n",
      "Iteration 409, loss = 0.48223006\n",
      "Iteration 410, loss = 0.48003784\n",
      "Iteration 411, loss = 0.48101161\n",
      "Iteration 412, loss = 0.48003510\n",
      "Iteration 413, loss = 0.48194267\n",
      "Iteration 414, loss = 0.48116324\n",
      "Iteration 415, loss = 0.48182813\n",
      "Iteration 416, loss = 0.48107419\n",
      "Iteration 417, loss = 0.48211703\n",
      "Iteration 418, loss = 0.48026460\n",
      "Iteration 419, loss = 0.48137260\n",
      "Iteration 420, loss = 0.47867719\n",
      "Iteration 421, loss = 0.48302793\n",
      "Iteration 422, loss = 0.48020370\n",
      "Iteration 423, loss = 0.48015767\n",
      "Iteration 424, loss = 0.48078636\n",
      "Iteration 425, loss = 0.47876214\n",
      "Iteration 426, loss = 0.47771686\n",
      "Iteration 427, loss = 0.48080558\n",
      "Iteration 428, loss = 0.47932939\n",
      "Iteration 429, loss = 0.47912457\n",
      "Iteration 430, loss = 0.48178239\n",
      "Iteration 431, loss = 0.47767724\n",
      "Iteration 432, loss = 0.48061280\n",
      "Iteration 433, loss = 0.47851420\n",
      "Iteration 434, loss = 0.47752434\n",
      "Iteration 435, loss = 0.47813591\n",
      "Iteration 436, loss = 0.48069359\n",
      "Iteration 437, loss = 0.47962475\n",
      "Iteration 438, loss = 0.47917539\n",
      "Iteration 439, loss = 0.47843627\n",
      "Iteration 440, loss = 0.47807348\n",
      "Iteration 441, loss = 0.47890050\n",
      "Iteration 442, loss = 0.47824434\n",
      "Iteration 443, loss = 0.47844769\n",
      "Iteration 444, loss = 0.47989300\n",
      "Iteration 445, loss = 0.47782356\n",
      "Iteration 446, loss = 0.47783431\n",
      "Iteration 447, loss = 0.47891351\n",
      "Iteration 448, loss = 0.47978209\n",
      "Iteration 449, loss = 0.47780979\n",
      "Iteration 450, loss = 0.47802034\n",
      "Iteration 451, loss = 0.47719837\n",
      "Iteration 452, loss = 0.47793785\n",
      "Iteration 453, loss = 0.48058461\n",
      "Iteration 454, loss = 0.48067948\n",
      "Iteration 455, loss = 0.47684429\n",
      "Iteration 456, loss = 0.47920596\n",
      "Iteration 457, loss = 0.47795347\n",
      "Iteration 458, loss = 0.47679678\n",
      "Iteration 459, loss = 0.47613166\n",
      "Iteration 460, loss = 0.47831903\n",
      "Iteration 461, loss = 0.47740833\n",
      "Iteration 462, loss = 0.47650646\n",
      "Iteration 463, loss = 0.47684112\n",
      "Iteration 464, loss = 0.47682210\n",
      "Iteration 465, loss = 0.47849438\n",
      "Iteration 466, loss = 0.47847550\n",
      "Iteration 467, loss = 0.47684060\n",
      "Iteration 468, loss = 0.47648966\n",
      "Iteration 469, loss = 0.48295490\n",
      "Iteration 470, loss = 0.47835519\n",
      "Iteration 471, loss = 0.47931660\n",
      "Iteration 472, loss = 0.47775529\n",
      "Iteration 473, loss = 0.47774890\n",
      "Iteration 474, loss = 0.47877104\n",
      "Iteration 475, loss = 0.47651489\n",
      "Iteration 476, loss = 0.47677956\n",
      "Iteration 477, loss = 0.47673531\n",
      "Iteration 478, loss = 0.47659235\n",
      "Iteration 479, loss = 0.47812386\n",
      "Iteration 480, loss = 0.47767576\n",
      "Iteration 481, loss = 0.47594553\n",
      "Iteration 482, loss = 0.47644379\n",
      "Iteration 483, loss = 0.47535864\n",
      "Iteration 484, loss = 0.47504053\n",
      "Iteration 485, loss = 0.47543149\n",
      "Iteration 486, loss = 0.47762437\n",
      "Iteration 487, loss = 0.47639703\n",
      "Iteration 488, loss = 0.47830832\n",
      "Iteration 489, loss = 0.47736894\n",
      "Iteration 490, loss = 0.47398517\n",
      "Iteration 491, loss = 0.47467820\n",
      "Iteration 492, loss = 0.47719013\n",
      "Iteration 493, loss = 0.47472338\n",
      "Iteration 494, loss = 0.47609669\n",
      "Iteration 495, loss = 0.47331690\n",
      "Iteration 496, loss = 0.47466157\n",
      "Iteration 497, loss = 0.47427695\n",
      "Iteration 498, loss = 0.47495337\n",
      "Iteration 499, loss = 0.47536705\n",
      "Iteration 500, loss = 0.47444185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arneish/.local/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
       "       learning_rate_init=0.01, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=100, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NN classifier for train-validation perf:\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),learning_rate='constant',learning_rate_init=0.01,solver='sgd',n_iter_no_change=100,max_iter=500,verbose=True)\n",
    "mlp.fit(local_train_modified,y_local_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation a/c: 55.78098943857699\n"
     ]
    }
   ],
   "source": [
    "#NN predictor:\n",
    "y_validation_pred = mlp.predict(local_validation_modified)\n",
    "count_match = 0\n",
    "count_error = 0\n",
    "assert(len(y_validation_pred)==len(y_local_validation))\n",
    "validation_gtruth=np.asarray(y_local_validation)\n",
    "for i in range(len(y_local_validation)):\n",
    "    if (int(y_validation_pred[i])==int(validation_gtruth[i])):\n",
    "        count_match+=1\n",
    "    else:\n",
    "        count_error+=1\n",
    "validation_accuracy = count_match/(count_match+count_error)*100.0\n",
    "print(\"validation a/c:\", validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 24429\n",
      "Test size: 6268\n"
     ]
    }
   ],
   "source": [
    "#RF classifier for train-test perf:\n",
    "clf2 = RandomForestClassifier(n_jobs=2, n_estimators=100, random_state=0)\n",
    "clf2.fit(train_modified, y_train)\n",
    "# y_test_pred = clf2.predict(test_modified)\n",
    "p = clf2.predict_proba(test_modified)\n",
    "prediction = []\n",
    "for x,y in p:\n",
    "    prediction.append(y)\n",
    "count_match = 0\n",
    "count_error = 0\n",
    "\n",
    "# assert(len(y_test_pred)==len(y_local_validation))\n",
    "# validation_gtruth=np.asarray(y_local_validation)\n",
    "# for i in range(len(y_local_validation)):\n",
    "#     if (int(y_validation_pred[i])==int(validation_gtruth[i])):\n",
    "#         count_match+=1\n",
    "#     else:\n",
    "#         count_error+=1\n",
    "# validation_accuracy = count_match/(count_match+count_error)*100.0\n",
    "# print(\"validation a/c:\", validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6268\n"
     ]
    }
   ],
   "source": [
    "#write outputs:\n",
    "# shot_arr = np.asarray(test['shot_id_number'])\n",
    "test_rows = data.index[data.is_goal.isnull()]\n",
    "count = 0\n",
    "f = open(\"submissionAP.csv\",\"w+\")\n",
    "print(\"shot_id_number,is_goal\", file=f)\n",
    "# print(\"shot_id_number,is_goal\", file=f)\n",
    "for i in range(len(test_rows)):\n",
    "    print(str(int(test_rows[i]+1))+\",\"+str(prediction[i]), file=f)\n",
    "    count+=1\n",
    "print(count)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(validation_gtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6268\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     importance\n",
      "match_event_id                     4.062614e-02\n",
      "match_id                           4.043662e-02\n",
      "distance_of_shot.1                 3.886515e-02\n",
      "location_y                         3.806000e-02\n",
      "remaining_sec                      3.752482e-02\n",
      "team_id                            3.705409e-02\n",
      "location_x                         3.665255e-02\n",
      "game_season_1996-97                3.538272e-02\n",
      "power_of_shot.1                    3.279888e-02\n",
      "remaining_min                      2.895486e-02\n",
      "distance_of_shot                   2.892361e-02\n",
      "knockout_match.1                   2.569584e-02\n",
      "remaining_sec.1                    1.979802e-02\n",
      "power_of_shot                      1.720950e-02\n",
      "shot_basics_Goal Line              8.132400e-03\n",
      "type_of_combined_shot_shot - 4     7.947864e-03\n",
      "lat/lng_43.062206, -87.944754      7.935073e-03\n",
      "area_of_shot_Left Side Center(LC)  6.941969e-03\n",
      "knockout_match                     6.820156e-03\n",
      "type_of_combined_shot_shot - 2     6.660315e-03\n",
      "type_of_shot_shot - 4              6.354727e-03\n",
      "team_name_Manchester United        5.891105e-03\n",
      "type_of_shot_shot - 45             4.710459e-03\n",
      "shot_basics_Penalty Spot           4.422066e-03\n",
      "date_of_game_1996-11-03            4.089886e-03\n",
      "range_of_shot_24+ ft.              3.878400e-03\n",
      "range_of_shot_8-16 ft.             3.805403e-03\n",
      "game_season_2006-07                3.747011e-03\n",
      "game_season_2009-10                3.702067e-03\n",
      "game_season_2010-11                3.690102e-03\n",
      "...                                         ...\n",
      "date_of_game_1997-02-21            1.367346e-05\n",
      "date_of_game_1998-03-08            1.285668e-05\n",
      "date_of_game_1996-11-17            1.255063e-05\n",
      "date_of_game_1997-04-30            1.244626e-05\n",
      "date_of_game_2001-04-10            1.242329e-05\n",
      "date_of_game_1997-01-14            1.236430e-05\n",
      "date_of_game_1997-02-19            1.217913e-05\n",
      "date_of_game_1998-05-24            1.070294e-05\n",
      "date_of_game_2013-03-22            1.015017e-05\n",
      "date_of_game_1997-03-26            1.000447e-05\n",
      "date_of_game_2005-02-13            9.766686e-06\n",
      "date_of_game_1997-03-07            9.660375e-06\n",
      "date_of_game_1997-01-03            9.474148e-06\n",
      "date_of_game_1998-03-28            9.042887e-06\n",
      "date_of_game_1996-12-29            8.820465e-06\n",
      "date_of_game_1998-05-16            8.689258e-06\n",
      "date_of_game_1996-12-17            8.199277e-06\n",
      "date_of_game_1996-11-13            6.978199e-06\n",
      "date_of_game_1996-11-29            6.408433e-06\n",
      "date_of_game_1997-04-25            4.887683e-06\n",
      "date_of_game_1996-12-04            4.847108e-06\n",
      "date_of_game_2000-06-14            4.759020e-06\n",
      "date_of_game_1996-11-20            4.426412e-06\n",
      "date_of_game_1996-11-06            3.306196e-06\n",
      "date_of_game_1997-05-08            3.034806e-06\n",
      "date_of_game_1997-03-23            2.695266e-06\n",
      "date_of_game_1997-02-28            2.573695e-06\n",
      "date_of_game_1998-03-18            3.050041e-07\n",
      "date_of_game_1997-01-02            0.000000e+00\n",
      "remaining_min.1                    0.000000e+00\n",
      "\n",
      "[1787 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# list(zip(train[features],clf2.feature_importances_))\n",
    "features_imp = pd.DataFrame(clf.feature_importances_, index=features,columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(features_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10531942 -0.67083244 -0.01123668 ... -0.78672968 -0.30885504\n",
      "  -0.05429942]\n",
      " [ 0.30404697 -0.37358089 -0.01123668 ... -0.78672968 -0.30885504\n",
      "  -0.05429942]\n",
      " [ 0.40683708 -0.94021666  1.71011053 ... -0.78672968 -0.30885504\n",
      "  -0.05429942]\n",
      " ...\n",
      " [ 0.06420337 -0.14135312 -0.01123668 ...  1.27108462 -0.30885504\n",
      "  -0.05429942]\n",
      " [ 0.52333254 -0.17850956  6.86167712 ...  1.27108462 -0.30885504\n",
      "  -0.05429942]\n",
      " [ 1.35935879  0.22092221  2.04887568 ... -0.78672968 -0.30885504\n",
      "  -0.05429942]]\n"
     ]
    }
   ],
   "source": [
    "print(local_validation_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([    0,     7,    16,    19,    21,    32,    33,    34,    35,\n",
      "               36,\n",
      "            ...\n",
      "            30659, 30664, 30668, 30679, 30680, 30681, 30682, 30686, 30687,\n",
      "            30693],\n",
      "           dtype='int64', length=6268)\n"
     ]
    }
   ],
   "source": [
    "print(test_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
